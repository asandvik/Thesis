{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f11362ff-f275-4ea6-a3e7-bc093b6119b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import av\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "def plot(imgs, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            img = F.to_pil_image(img.to(\"cpu\"))\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8331b59-49fa-4535-aab6-47714f92b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: L334aqEJxys01.mp4\n",
      "Frame number: 145\n",
      "Frame label: 1\n"
     ]
    }
   ],
   "source": [
    "frame_list = pd.read_csv('anno_train.csv', header=None)\n",
    "\n",
    "n = 3\n",
    "video_name = frame_list.iloc[n, 0]\n",
    "frame_num = frame_list.iloc[n, 1]\n",
    "frame_label = frame_list.iloc[n, 2]\n",
    "\n",
    "print('Video name: {}'.format(video_name))\n",
    "print('Frame number: {}'.format(frame_num))\n",
    "print('Frame label: {}'.format(frame_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "928fc35a-22d7-4395-884a-83761d9b70a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/data/Datasets/CPTAD/Videos/L334aqEJxys01.mp4\n",
      "torch.Size([1, 3, 720, 1280])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/envs/data-science-stack-2.11.0/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m frames,_,_ \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_video(filename,start_pts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m144\u001b[39m,end_pts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,pts_unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpts\u001b[39m\u001b[38;5;124m'\u001b[39m,output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTCHW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(frames\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m----> 8\u001b[0m im_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([frames[\u001b[38;5;241m0\u001b[39m], \u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[1;32m     10\u001b[0m plot(im_batch)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# _,ret = cv2.imencode('.jpg', frames[0])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# i = IPython.display.Image(data=ret)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# IPython.display.display(i)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#todo: show clip\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "filename = '/notebooks/data/Datasets/CPTAD/Videos/' + video_name\n",
    "print(filename)\n",
    "nframes = 10\n",
    "\n",
    "# torchvision loader\n",
    "frames,_,_ = torchvision.io.read_video(filename,start_pts=144/30,end_pts=150/30,pts_unit='sec',output_format=\"TCHW\")\n",
    "print(frames.size())\n",
    "im_batch = torch.stack([frames[0], frames[6]])\n",
    " \n",
    "plot(im_batch)\n",
    "\n",
    "# _,ret = cv2.imencode('.jpg', frames[0])\n",
    "# i = IPython.display.Image(data=ret)\n",
    "# IPython.display.display(i)\n",
    "\n",
    "# opencv loader\n",
    "# frames = []\n",
    "# cap = cv2.VideoCapture(filename)\n",
    "# #cap.set(2, frame_num - nframes) # CAP_PROP_POS_FRAMES\n",
    "# ret = True\n",
    "# i = 0\n",
    "# while ret and i < nframes:\n",
    "#     ret, img = cap.read()\n",
    "#     if ret:\n",
    "#         frames.append(img)\n",
    "#         i += 1\n",
    "# clip = np.stack(frames, axis=0)\n",
    "\n",
    "#todo: show clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aea52e-27d7-4ef7-ad64-5cefb102ad51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
