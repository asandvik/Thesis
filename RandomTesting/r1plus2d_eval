digraph {
	graph [size="105.14999999999999,105.14999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140681867942928 [label="
 (1, 400)" fillcolor=darkolivegreen1]
	140681923932416 [label=AddmmBackward0]
	140681866473680 -> 140681923932416
	140681868262224 [label="fc.bias
 (400)" fillcolor=lightblue]
	140681868262224 -> 140681866473680
	140681866473680 [label=AccumulateGrad]
	140681866468352 -> 140681923932416
	140681866468352 [label=ViewBackward0]
	140681866466336 -> 140681866468352
	140681866466336 [label=MeanBackward1]
	140681866463696 -> 140681866466336
	140681866463696 [label=ReluBackward0]
	140681866471952 -> 140681866463696
	140681866471952 [label=AddBackward0]
	140681866476656 -> 140681866471952
	140681866476656 [label=NativeBatchNormBackward0]
	140681866471376 -> 140681866476656
	140681866471376 [label=ConvolutionBackward0]
	140681866461296 -> 140681866471376
	140681866461296 [label=ReluBackward0]
	140681866472096 -> 140681866461296
	140681866472096 [label=NativeBatchNormBackward0]
	140681866475264 -> 140681866472096
	140681866475264 [label=ConvolutionBackward0]
	140681866471760 -> 140681866475264
	140681866471760 [label=ReluBackward0]
	140681866468640 -> 140681866471760
	140681866468640 [label=NativeBatchNormBackward0]
	140681866474208 -> 140681866468640
	140681866474208 [label=ConvolutionBackward0]
	140681866474016 -> 140681866474208
	140681866474016 [label=ReluBackward0]
	140681866474448 -> 140681866474016
	140681866474448 [label=NativeBatchNormBackward0]
	140681923046096 -> 140681866474448
	140681923046096 [label=ConvolutionBackward0]
	140681866469888 -> 140681923046096
	140681866469888 [label=ReluBackward0]
	140681923038656 -> 140681866469888
	140681923038656 [label=AddBackward0]
	140681868285488 -> 140681923038656
	140681868285488 [label=NativeBatchNormBackward0]
	140681868286640 -> 140681868285488
	140681868286640 [label=ConvolutionBackward0]
	140681868287744 -> 140681868286640
	140681868287744 [label=ReluBackward0]
	140681868290096 -> 140681868287744
	140681868290096 [label=NativeBatchNormBackward0]
	140681868287984 -> 140681868290096
	140681868287984 [label=ConvolutionBackward0]
	140681868292304 -> 140681868287984
	140681868292304 [label=ReluBackward0]
	140681868295520 -> 140681868292304
	140681868295520 [label=NativeBatchNormBackward0]
	140681868293936 -> 140681868295520
	140681868293936 [label=ConvolutionBackward0]
	140681868294848 -> 140681868293936
	140681868294848 [label=ReluBackward0]
	140681868290384 -> 140681868294848
	140681868290384 [label=NativeBatchNormBackward0]
	140681868291824 -> 140681868290384
	140681868291824 [label=ConvolutionBackward0]
	140681868286208 -> 140681868291824
	140681868286208 [label=ReluBackward0]
	140681868288944 -> 140681868286208
	140681868288944 [label=AddBackward0]
	140681868282176 -> 140681868288944
	140681868282176 [label=NativeBatchNormBackward0]
	140681868283472 -> 140681868282176
	140681868283472 [label=ConvolutionBackward0]
	140681868280976 -> 140681868283472
	140681868280976 [label=ReluBackward0]
	140681868289952 -> 140681868280976
	140681868289952 [label=NativeBatchNormBackward0]
	140681868295760 -> 140681868289952
	140681868295760 [label=ConvolutionBackward0]
	140681868287504 -> 140681868295760
	140681868287504 [label=ReluBackward0]
	140681868284528 -> 140681868287504
	140681868284528 [label=NativeBatchNormBackward0]
	140681868287360 -> 140681868284528
	140681868287360 [label=ConvolutionBackward0]
	140681868284672 -> 140681868287360
	140681868284672 [label=ReluBackward0]
	140681868286976 -> 140681868284672
	140681868286976 [label=NativeBatchNormBackward0]
	140681868290432 -> 140681868286976
	140681868290432 [label=ConvolutionBackward0]
	140681868284096 -> 140681868290432
	140681868284096 [label=ReluBackward0]
	140681868295088 -> 140681868284096
	140681868295088 [label=AddBackward0]
	140681868293264 -> 140681868295088
	140681868293264 [label=NativeBatchNormBackward0]
	140681868293408 -> 140681868293264
	140681868293408 [label=ConvolutionBackward0]
	140681868283712 -> 140681868293408
	140681868283712 [label=ReluBackward0]
	140681868286304 -> 140681868283712
	140681868286304 [label=NativeBatchNormBackward0]
	140681868286016 -> 140681868286304
	140681868286016 [label=ConvolutionBackward0]
	140681868283760 -> 140681868286016
	140681868283760 [label=ReluBackward0]
	140681868283328 -> 140681868283760
	140681868283328 [label=NativeBatchNormBackward0]
	140681868282416 -> 140681868283328
	140681868282416 [label=ConvolutionBackward0]
	140681868282224 -> 140681868282416
	140681868282224 [label=ReluBackward0]
	140681868281168 -> 140681868282224
	140681868281168 [label=NativeBatchNormBackward0]
	140681868280592 -> 140681868281168
	140681868280592 [label=ConvolutionBackward0]
	140681868295136 -> 140681868280592
	140681868295136 [label=ReluBackward0]
	140681868294800 -> 140681868295136
	140681868294800 [label=AddBackward0]
	140681868293744 -> 140681868294800
	140681868293744 [label=NativeBatchNormBackward0]
	140681868292592 -> 140681868293744
	140681868292592 [label=ConvolutionBackward0]
	140681868291632 -> 140681868292592
	140681868291632 [label=ReluBackward0]
	140681868291296 -> 140681868291632
	140681868291296 [label=NativeBatchNormBackward0]
	140681868290480 -> 140681868291296
	140681868290480 [label=ConvolutionBackward0]
	140681868290192 -> 140681868290480
	140681868290192 [label=ReluBackward0]
	140681868289280 -> 140681868290192
	140681868289280 [label=NativeBatchNormBackward0]
	140681868289184 -> 140681868289280
	140681868289184 [label=ConvolutionBackward0]
	140681868288032 -> 140681868289184
	140681868288032 [label=ReluBackward0]
	140681868287792 -> 140681868288032
	140681868287792 [label=NativeBatchNormBackward0]
	140681868286928 -> 140681868287792
	140681868286928 [label=ConvolutionBackward0]
	140681868293888 -> 140681868286928
	140681868293888 [label=ReluBackward0]
	140681868286496 -> 140681868293888
	140681868286496 [label=AddBackward0]
	140681868286256 -> 140681868286496
	140681868286256 [label=NativeBatchNormBackward0]
	140681868284480 -> 140681868286256
	140681868284480 [label=ConvolutionBackward0]
	140681868284240 -> 140681868284480
	140681868284240 [label=ReluBackward0]
	140681868283520 -> 140681868284240
	140681868283520 [label=NativeBatchNormBackward0]
	140681868283376 -> 140681868283520
	140681868283376 [label=ConvolutionBackward0]
	140681868282464 -> 140681868283376
	140681868282464 [label=ReluBackward0]
	140681868282272 -> 140681868282464
	140681868282272 [label=NativeBatchNormBackward0]
	140681868280784 -> 140681868282272
	140681868280784 [label=ConvolutionBackward0]
	140681868280448 -> 140681868280784
	140681868280448 [label=ReluBackward0]
	140681868284912 -> 140681868280448
	140681868284912 [label=NativeBatchNormBackward0]
	140681868285344 -> 140681868284912
	140681868285344 [label=ConvolutionBackward0]
	140681868292256 -> 140681868285344
	140681868292256 [label=ReluBackward0]
	140681868292016 -> 140681868292256
	140681868292016 [label=AddBackward0]
	140681868296144 -> 140681868292016
	140681868296144 [label=NativeBatchNormBackward0]
	140681868295904 -> 140681868296144
	140681868295904 [label=ConvolutionBackward0]
	140681868295808 -> 140681868295904
	140681868295808 [label=ReluBackward0]
	140681868294608 -> 140681868295808
	140681868294608 [label=NativeBatchNormBackward0]
	140681868293360 -> 140681868294608
	140681868293360 [label=ConvolutionBackward0]
	140681868291968 -> 140681868293360
	140681868291968 [label=ReluBackward0]
	140681868290528 -> 140681868291968
	140681868290528 [label=NativeBatchNormBackward0]
	140681868289520 -> 140681868290528
	140681868289520 [label=ConvolutionBackward0]
	140681868288272 -> 140681868289520
	140681868288272 [label=ReluBackward0]
	140681868286832 -> 140681868288272
	140681868286832 [label=NativeBatchNormBackward0]
	140681868285824 -> 140681868286832
	140681868285824 [label=ConvolutionBackward0]
	140681868292736 -> 140681868285824
	140681868292736 [label=ReluBackward0]
	140681868282560 -> 140681868292736
	140681868282560 [label=AddBackward0]
	140681868281648 -> 140681868282560
	140681868281648 [label=NativeBatchNormBackward0]
	140681868280400 -> 140681868281648
	140681868280400 [label=ConvolutionBackward0]
	140681868294416 -> 140681868280400
	140681868294416 [label=ReluBackward0]
	140681868293120 -> 140681868294416
	140681868293120 [label=NativeBatchNormBackward0]
	140681868291920 -> 140681868293120
	140681868291920 [label=ConvolutionBackward0]
	140681868290720 -> 140681868291920
	140681868290720 [label=ReluBackward0]
	140681868289616 -> 140681868290720
	140681868289616 [label=NativeBatchNormBackward0]
	140681868288368 -> 140681868289616
	140681868288368 [label=ConvolutionBackward0]
	140681868287216 -> 140681868288368
	140681868287216 [label=ReluBackward0]
	140681868285968 -> 140681868287216
	140681868285968 [label=NativeBatchNormBackward0]
	140681868284768 -> 140681868285968
	140681868284768 [label=ConvolutionBackward0]
	140681868282992 -> 140681868284768
	140681868282992 [label=ReluBackward0]
	140681868282368 -> 140681868282992
	140681868282368 [label=NativeBatchNormBackward0]
	140681868280736 -> 140681868282368
	140681868280736 [label=ConvolutionBackward0]
	140681868284192 -> 140681868280736
	140681868284192 [label=ReluBackward0]
	140681868292208 -> 140681868284192
	140681868292208 [label=NativeBatchNormBackward0]
	140681868296096 -> 140681868292208
	140681868296096 [label=ConvolutionBackward0]
	140681868288608 -> 140681868296096
	140681867563936 [label="stem.0.weight
 (45, 3, 1, 7, 7)" fillcolor=lightblue]
	140681867563936 -> 140681868288608
	140681868288608 [label=AccumulateGrad]
	140681868289376 -> 140681868292208
	140681867570256 [label="stem.1.weight
 (45)" fillcolor=lightblue]
	140681867570256 -> 140681868289376
	140681868289376 [label=AccumulateGrad]
	140681868290864 -> 140681868292208
	140681867569696 [label="stem.1.bias
 (45)" fillcolor=lightblue]
	140681867569696 -> 140681868290864
	140681868290864 [label=AccumulateGrad]
	140681868291248 -> 140681868280736
	140681867564256 [label="stem.3.weight
 (64, 45, 3, 1, 1)" fillcolor=lightblue]
	140681867564256 -> 140681868291248
	140681868291248 [label=AccumulateGrad]
	140681868292544 -> 140681868282368
	140681867566096 [label="stem.4.weight
 (64)" fillcolor=lightblue]
	140681867566096 -> 140681868292544
	140681868292544 [label=AccumulateGrad]
	140681868294128 -> 140681868282368
	140681867564416 [label="stem.4.bias
 (64)" fillcolor=lightblue]
	140681867564416 -> 140681868294128
	140681868294128 [label=AccumulateGrad]
	140681868293072 -> 140681868284768
	140681867564736 [label="layer1.0.conv1.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140681867564736 -> 140681868293072
	140681868293072 [label=AccumulateGrad]
	140681868295184 -> 140681868285968
	140681867567216 [label="layer1.0.conv1.0.1.weight
 (144)" fillcolor=lightblue]
	140681867567216 -> 140681868295184
	140681868295184 [label=AccumulateGrad]
	140681868295328 -> 140681868285968
	140681867566256 [label="layer1.0.conv1.0.1.bias
 (144)" fillcolor=lightblue]
	140681867566256 -> 140681868295328
	140681868295328 [label=AccumulateGrad]
	140681868295280 -> 140681868288368
	140681867564576 [label="layer1.0.conv1.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140681867564576 -> 140681868295280
	140681868295280 [label=AccumulateGrad]
	140681868294464 -> 140681868289616
	140681867568016 [label="layer1.0.conv1.1.weight
 (64)" fillcolor=lightblue]
	140681867568016 -> 140681868294464
	140681868294464 [label=AccumulateGrad]
	140681868293648 -> 140681868289616
	140681867571696 [label="layer1.0.conv1.1.bias
 (64)" fillcolor=lightblue]
	140681867571696 -> 140681868293648
	140681868293648 [label=AccumulateGrad]
	140681868291392 -> 140681868291920
	140681867571616 [label="layer1.0.conv2.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140681867571616 -> 140681868291392
	140681868291392 [label=AccumulateGrad]
	140681868289712 -> 140681868293120
	140681867567936 [label="layer1.0.conv2.0.1.weight
 (144)" fillcolor=lightblue]
	140681867567936 -> 140681868289712
	140681868289712 [label=AccumulateGrad]
	140681868290816 -> 140681868293120
	140681867571936 [label="layer1.0.conv2.0.1.bias
 (144)" fillcolor=lightblue]
	140681867571936 -> 140681868290816
	140681868290816 [label=AccumulateGrad]
	140681868288992 -> 140681868280400
	140681867570096 [label="layer1.0.conv2.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140681867570096 -> 140681868288992
	140681868288992 [label=AccumulateGrad]
	140681868288512 -> 140681868281648
	140681917539936 [label="layer1.0.conv2.1.weight
 (64)" fillcolor=lightblue]
	140681917539936 -> 140681868288512
	140681868288512 [label=AccumulateGrad]
	140681868287696 -> 140681868281648
	140681917536816 [label="layer1.0.conv2.1.bias
 (64)" fillcolor=lightblue]
	140681917536816 -> 140681868287696
	140681868287696 [label=AccumulateGrad]
	140681868282992 -> 140681868282560
	140681868287072 -> 140681868285824
	140681869618976 [label="layer1.1.conv1.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140681869618976 -> 140681868287072
	140681868287072 [label=AccumulateGrad]
	140681868284048 -> 140681868286832
	140681869617856 [label="layer1.1.conv1.0.1.weight
 (144)" fillcolor=lightblue]
	140681869617856 -> 140681868284048
	140681868284048 [label=AccumulateGrad]
	140681868282704 -> 140681868286832
	140681869618256 [label="layer1.1.conv1.0.1.bias
 (144)" fillcolor=lightblue]
	140681869618256 -> 140681868282704
	140681868282704 [label=AccumulateGrad]
	140681868292640 -> 140681868289520
	140681869619856 [label="layer1.1.conv1.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140681869619856 -> 140681868292640
	140681868292640 [label=AccumulateGrad]
	140681868283088 -> 140681868290528
	140681869617136 [label="layer1.1.conv1.1.weight
 (64)" fillcolor=lightblue]
	140681869617136 -> 140681868283088
	140681868283088 [label=AccumulateGrad]
	140681868286352 -> 140681868290528
	140681869612896 [label="layer1.1.conv1.1.bias
 (64)" fillcolor=lightblue]
	140681869612896 -> 140681868286352
	140681868286352 [label=AccumulateGrad]
	140681868288800 -> 140681868293360
	140681869615456 [label="layer1.1.conv2.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140681869615456 -> 140681868288800
	140681868288800 [label=AccumulateGrad]
	140681868293552 -> 140681868294608
	140681869619536 [label="layer1.1.conv2.0.1.weight
 (144)" fillcolor=lightblue]
	140681869619536 -> 140681868293552
	140681868293552 [label=AccumulateGrad]
	140681868282320 -> 140681868294608
	140681869618816 [label="layer1.1.conv2.0.1.bias
 (144)" fillcolor=lightblue]
	140681869618816 -> 140681868282320
	140681868282320 [label=AccumulateGrad]
	140681868285008 -> 140681868295904
	140681869619376 [label="layer1.1.conv2.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140681869619376 -> 140681868285008
	140681868285008 [label=AccumulateGrad]
	140681868289808 -> 140681868296144
	140681869618016 [label="layer1.1.conv2.1.weight
 (64)" fillcolor=lightblue]
	140681869618016 -> 140681868289808
	140681868289808 [label=AccumulateGrad]
	140681868291200 -> 140681868296144
	140681869612176 [label="layer1.1.conv2.1.bias
 (64)" fillcolor=lightblue]
	140681869612176 -> 140681868291200
	140681868291200 [label=AccumulateGrad]
	140681868292736 -> 140681868292016
	140681868291584 -> 140681868285344
	140681869023472 [label="layer2.0.conv1.0.0.weight
 (230, 64, 1, 3, 3)" fillcolor=lightblue]
	140681869023472 -> 140681868291584
	140681868291584 [label=AccumulateGrad]
	140681868280832 -> 140681868284912
	140681869023152 [label="layer2.0.conv1.0.1.weight
 (230)" fillcolor=lightblue]
	140681869023152 -> 140681868280832
	140681868280832 [label=AccumulateGrad]
	140681868282896 -> 140681868284912
	140681869017792 [label="layer2.0.conv1.0.1.bias
 (230)" fillcolor=lightblue]
	140681869017792 -> 140681868282896
	140681868282896 [label=AccumulateGrad]
	140681868283664 -> 140681868280784
	140681869018912 [label="layer2.0.conv1.0.3.weight
 (128, 230, 3, 1, 1)" fillcolor=lightblue]
	140681869018912 -> 140681868283664
	140681868283664 [label=AccumulateGrad]
	140681868286160 -> 140681868282272
	140681869019712 [label="layer2.0.conv1.1.weight
 (128)" fillcolor=lightblue]
	140681869019712 -> 140681868286160
	140681868286160 [label=AccumulateGrad]
	140681868288224 -> 140681868282272
	140681869018752 [label="layer2.0.conv1.1.bias
 (128)" fillcolor=lightblue]
	140681869018752 -> 140681868288224
	140681868288224 [label=AccumulateGrad]
	140681868289424 -> 140681868283376
	140681869018112 [label="layer2.0.conv2.0.0.weight
 (230, 128, 1, 3, 3)" fillcolor=lightblue]
	140681869018112 -> 140681868289424
	140681868289424 [label=AccumulateGrad]
	140681868291728 -> 140681868283520
	140681869023072 [label="layer2.0.conv2.0.1.weight
 (230)" fillcolor=lightblue]
	140681869023072 -> 140681868291728
	140681868291728 [label=AccumulateGrad]
	140681868293312 -> 140681868283520
	140681953628128 [label="layer2.0.conv2.0.1.bias
 (230)" fillcolor=lightblue]
	140681953628128 -> 140681868293312
	140681868293312 [label=AccumulateGrad]
	140681868294704 -> 140681868284480
	140681869382960 [label="layer2.0.conv2.0.3.weight
 (128, 230, 3, 1, 1)" fillcolor=lightblue]
	140681869382960 -> 140681868294704
	140681868294704 [label=AccumulateGrad]
	140681868281984 -> 140681868286256
	140681869381920 [label="layer2.0.conv2.1.weight
 (128)" fillcolor=lightblue]
	140681869381920 -> 140681868281984
	140681868281984 [label=AccumulateGrad]
	140681868282944 -> 140681868286256
	140681869388080 [label="layer2.0.conv2.1.bias
 (128)" fillcolor=lightblue]
	140681869388080 -> 140681868282944
	140681868282944 [label=AccumulateGrad]
	140681868285728 -> 140681868286496
	140681868285728 [label=NativeBatchNormBackward0]
	140681868283280 -> 140681868285728
	140681868283280 [label=ConvolutionBackward0]
	140681868292256 -> 140681868283280
	140681868284816 -> 140681868283280
	140681873344048 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1, 1)" fillcolor=lightblue]
	140681873344048 -> 140681868284816
	140681868284816 [label=AccumulateGrad]
	140681868280160 -> 140681868285728
	140681873345008 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140681873345008 -> 140681868280160
	140681868280160 [label=AccumulateGrad]
	140681868280544 -> 140681868285728
	140681873344528 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140681873344528 -> 140681868280544
	140681868280544 [label=AccumulateGrad]
	140681868285920 -> 140681868286928
	140681869385040 [label="layer2.1.conv1.0.0.weight
 (288, 128, 1, 3, 3)" fillcolor=lightblue]
	140681869385040 -> 140681868285920
	140681868285920 [label=AccumulateGrad]
	140681868289472 -> 140681868287792
	140681869388720 [label="layer2.1.conv1.0.1.weight
 (288)" fillcolor=lightblue]
	140681869388720 -> 140681868289472
	140681868289472 [label=AccumulateGrad]
	140681868290672 -> 140681868287792
	140681869381680 [label="layer2.1.conv1.0.1.bias
 (288)" fillcolor=lightblue]
	140681869381680 -> 140681868290672
	140681868290672 [label=AccumulateGrad]
	140681868292112 -> 140681868289184
	140681869387920 [label="layer2.1.conv1.0.3.weight
 (128, 288, 3, 1, 1)" fillcolor=lightblue]
	140681869387920 -> 140681868292112
	140681868292112 [label=AccumulateGrad]
	140681868294752 -> 140681868289280
	140681869386160 [label="layer2.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	140681869386160 -> 140681868294752
	140681868294752 [label=AccumulateGrad]
	140681868294656 -> 140681868289280
	140681869383600 [label="layer2.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	140681869383600 -> 140681868294656
	140681868294656 [label=AccumulateGrad]
	140681868293840 -> 140681868290480
	140681869393200 [label="layer2.1.conv2.0.0.weight
 (288, 128, 1, 3, 3)" fillcolor=lightblue]
	140681869393200 -> 140681868293840
	140681868293840 [label=AccumulateGrad]
	140681868290576 -> 140681868291296
	140681869386880 [label="layer2.1.conv2.0.1.weight
 (288)" fillcolor=lightblue]
	140681869386880 -> 140681868290576
	140681868290576 [label=AccumulateGrad]
	140681868288752 -> 140681868291296
	140681869393440 [label="layer2.1.conv2.0.1.bias
 (288)" fillcolor=lightblue]
	140681869393440 -> 140681868288752
	140681868288752 [label=AccumulateGrad]
	140681868288080 -> 140681868292592
	140681869387200 [label="layer2.1.conv2.0.3.weight
 (128, 288, 3, 1, 1)" fillcolor=lightblue]
	140681869387200 -> 140681868288080
	140681868288080 [label=AccumulateGrad]
	140681868285056 -> 140681868293744
	140681869386720 [label="layer2.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	140681869386720 -> 140681868285056
	140681868285056 [label=AccumulateGrad]
	140681868284432 -> 140681868293744
	140681869387040 [label="layer2.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	140681869387040 -> 140681868284432
	140681868284432 [label=AccumulateGrad]
	140681868293888 -> 140681868294800
	140681921223584 -> 140681868280592
	140681869450176 [label="layer3.0.conv1.0.0.weight
 (460, 128, 1, 3, 3)" fillcolor=lightblue]
	140681869450176 -> 140681921223584
	140681921223584 [label=AccumulateGrad]
	140681921231168 -> 140681868281168
	140681869454256 [label="layer3.0.conv1.0.1.weight
 (460)" fillcolor=lightblue]
	140681869454256 -> 140681921231168
	140681921231168 [label=AccumulateGrad]
	140681921231504 -> 140681868281168
	140681869451856 [label="layer3.0.conv1.0.1.bias
 (460)" fillcolor=lightblue]
	140681869451856 -> 140681921231504
	140681921231504 [label=AccumulateGrad]
	140681921226272 -> 140681868282416
	140681869454496 [label="layer3.0.conv1.0.3.weight
 (256, 460, 3, 1, 1)" fillcolor=lightblue]
	140681869454496 -> 140681921226272
	140681921226272 [label=AccumulateGrad]
	140681921229392 -> 140681868283328
	140681869454416 [label="layer3.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	140681869454416 -> 140681921229392
	140681921229392 [label=AccumulateGrad]
	140681921223440 -> 140681868283328
	140681869448976 [label="layer3.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	140681869448976 -> 140681921223440
	140681921223440 [label=AccumulateGrad]
	140681867555168 -> 140681868286016
	140681869447856 [label="layer3.0.conv2.0.0.weight
 (460, 256, 1, 3, 3)" fillcolor=lightblue]
	140681869447856 -> 140681867555168
	140681867555168 [label=AccumulateGrad]
	140681867555072 -> 140681868286304
	140681869447776 [label="layer3.0.conv2.0.1.weight
 (460)" fillcolor=lightblue]
	140681869447776 -> 140681867555072
	140681867555072 [label=AccumulateGrad]
	140681867551904 -> 140681868286304
	140681869454736 [label="layer3.0.conv2.0.1.bias
 (460)" fillcolor=lightblue]
	140681869454736 -> 140681867551904
	140681867551904 [label=AccumulateGrad]
	140681867544416 -> 140681868293408
	140681869449856 [label="layer3.0.conv2.0.3.weight
 (256, 460, 3, 1, 1)" fillcolor=lightblue]
	140681869449856 -> 140681867544416
	140681867544416 [label=AccumulateGrad]
	140681867546480 -> 140681868293264
	140681869452816 [label="layer3.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	140681869452816 -> 140681867546480
	140681867546480 [label=AccumulateGrad]
	140681867546624 -> 140681868293264
	140681869457616 [label="layer3.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	140681869457616 -> 140681867546624
	140681867546624 [label=AccumulateGrad]
	140681868293504 -> 140681868295088
	140681868293504 [label=NativeBatchNormBackward0]
	140681868283952 -> 140681868293504
	140681868283952 [label=ConvolutionBackward0]
	140681868295136 -> 140681868283952
	140681921225552 -> 140681868283952
	140681869387120 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1, 1)" fillcolor=lightblue]
	140681869387120 -> 140681921225552
	140681921225552 [label=AccumulateGrad]
	140681867556080 -> 140681868293504
	140681869380400 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140681869380400 -> 140681867556080
	140681867556080 [label=AccumulateGrad]
	140681867543936 -> 140681868293504
	140681922600272 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140681922600272 -> 140681867543936
	140681867543936 [label=AccumulateGrad]
	140681867558720 -> 140681868290432
	140681869489504 [label="layer3.1.conv1.0.0.weight
 (576, 256, 1, 3, 3)" fillcolor=lightblue]
	140681869489504 -> 140681867558720
	140681867558720 [label=AccumulateGrad]
	140681867558384 -> 140681868286976
	140681869488144 [label="layer3.1.conv1.0.1.weight
 (576)" fillcolor=lightblue]
	140681869488144 -> 140681867558384
	140681867558384 [label=AccumulateGrad]
	140681867558048 -> 140681868286976
	140681869488064 [label="layer3.1.conv1.0.1.bias
 (576)" fillcolor=lightblue]
	140681869488064 -> 140681867558048
	140681867558048 [label=AccumulateGrad]
	140681867558480 -> 140681868287360
	140681869486304 [label="layer3.1.conv1.0.3.weight
 (256, 576, 3, 1, 1)" fillcolor=lightblue]
	140681869486304 -> 140681867558480
	140681867558480 [label=AccumulateGrad]
	140681867557088 -> 140681868284528
	140681869486384 [label="layer3.1.conv1.1.weight
 (256)" fillcolor=lightblue]
	140681869486384 -> 140681867557088
	140681867557088 [label=AccumulateGrad]
	140681867547728 -> 140681868284528
	140681869487984 [label="layer3.1.conv1.1.bias
 (256)" fillcolor=lightblue]
	140681869487984 -> 140681867547728
	140681867547728 [label=AccumulateGrad]
	140681866473536 -> 140681868295760
	140681869487424 [label="layer3.1.conv2.0.0.weight
 (576, 256, 1, 3, 3)" fillcolor=lightblue]
	140681869487424 -> 140681866473536
	140681866473536 [label=AccumulateGrad]
	140681866474976 -> 140681868289952
	140681869486544 [label="layer3.1.conv2.0.1.weight
 (576)" fillcolor=lightblue]
	140681869486544 -> 140681866474976
	140681866474976 [label=AccumulateGrad]
	140681866476560 -> 140681868289952
	140681869481664 [label="layer3.1.conv2.0.1.bias
 (576)" fillcolor=lightblue]
	140681869481664 -> 140681866476560
	140681866476560 [label=AccumulateGrad]
	140681866475936 -> 140681868283472
	140681869479664 [label="layer3.1.conv2.0.3.weight
 (256, 576, 3, 1, 1)" fillcolor=lightblue]
	140681869479664 -> 140681866475936
	140681866475936 [label=AccumulateGrad]
	140681866465424 -> 140681868282176
	140681869483264 [label="layer3.1.conv2.1.weight
 (256)" fillcolor=lightblue]
	140681869483264 -> 140681866465424
	140681866465424 [label=AccumulateGrad]
	140681866477472 -> 140681868282176
	140681869479264 [label="layer3.1.conv2.1.bias
 (256)" fillcolor=lightblue]
	140681869479264 -> 140681866477472
	140681866477472 [label=AccumulateGrad]
	140681868284096 -> 140681868288944
	140681866476176 -> 140681868291824
	140681869485264 [label="layer4.0.conv1.0.0.weight
 (921, 256, 1, 3, 3)" fillcolor=lightblue]
	140681869485264 -> 140681866476176
	140681866476176 [label=AccumulateGrad]
	140681866462064 -> 140681868290384
	140681869491424 [label="layer4.0.conv1.0.1.weight
 (921)" fillcolor=lightblue]
	140681869491424 -> 140681866462064
	140681866462064 [label=AccumulateGrad]
	140681866461728 -> 140681868290384
	140681869479504 [label="layer4.0.conv1.0.1.bias
 (921)" fillcolor=lightblue]
	140681869479504 -> 140681866461728
	140681866461728 [label=AccumulateGrad]
	140681866462448 -> 140681868293936
	140681869479584 [label="layer4.0.conv1.0.3.weight
 (512, 921, 3, 1, 1)" fillcolor=lightblue]
	140681869479584 -> 140681866462448
	140681866462448 [label=AccumulateGrad]
	140681866464656 -> 140681868295520
	140682308196352 [label="layer4.0.conv1.1.weight
 (512)" fillcolor=lightblue]
	140682308196352 -> 140681866464656
	140681866464656 [label=AccumulateGrad]
	140681866463456 -> 140681868295520
	140681868255424 [label="layer4.0.conv1.1.bias
 (512)" fillcolor=lightblue]
	140681868255424 -> 140681866463456
	140681866463456 [label=AccumulateGrad]
	140681866463792 -> 140681868287984
	140681868256304 [label="layer4.0.conv2.0.0.weight
 (921, 512, 1, 3, 3)" fillcolor=lightblue]
	140681868256304 -> 140681866463792
	140681866463792 [label=AccumulateGrad]
	140681866464464 -> 140681868290096
	140681868257504 [label="layer4.0.conv2.0.1.weight
 (921)" fillcolor=lightblue]
	140681868257504 -> 140681866464464
	140681866464464 [label=AccumulateGrad]
	140681866472672 -> 140681868290096
	140681868257904 [label="layer4.0.conv2.0.1.bias
 (921)" fillcolor=lightblue]
	140681868257904 -> 140681866472672
	140681866472672 [label=AccumulateGrad]
	140681866465136 -> 140681868286640
	140681868258704 [label="layer4.0.conv2.0.3.weight
 (512, 921, 3, 1, 1)" fillcolor=lightblue]
	140681868258704 -> 140681866465136
	140681866465136 [label=AccumulateGrad]
	140681866476464 -> 140681868285488
	140681868258464 [label="layer4.0.conv2.1.weight
 (512)" fillcolor=lightblue]
	140681868258464 -> 140681866476464
	140681866476464 [label=AccumulateGrad]
	140681866476368 -> 140681868285488
	140681868258544 [label="layer4.0.conv2.1.bias
 (512)" fillcolor=lightblue]
	140681868258544 -> 140681866476368
	140681866476368 [label=AccumulateGrad]
	140681868284576 -> 140681923038656
	140681868284576 [label=NativeBatchNormBackward0]
	140681868293696 -> 140681868284576
	140681868293696 [label=ConvolutionBackward0]
	140681868286208 -> 140681868293696
	140681866464800 -> 140681868293696
	140681869482624 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1, 1)" fillcolor=lightblue]
	140681869482624 -> 140681866464800
	140681866464800 [label=AccumulateGrad]
	140681866464080 -> 140681868284576
	140681869482304 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140681869482304 -> 140681866464080
	140681866464080 [label=AccumulateGrad]
	140681866465328 -> 140681868284576
	140681869479104 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140681869479104 -> 140681866465328
	140681866465328 [label=AccumulateGrad]
	140681866473152 -> 140681923046096
	140681868259664 [label="layer4.1.conv1.0.0.weight
 (1152, 512, 1, 3, 3)" fillcolor=lightblue]
	140681868259664 -> 140681866473152
	140681866473152 [label=AccumulateGrad]
	140681866470992 -> 140681866474448
	140681868259904 [label="layer4.1.conv1.0.1.weight
 (1152)" fillcolor=lightblue]
	140681868259904 -> 140681866470992
	140681866470992 [label=AccumulateGrad]
	140681866476944 -> 140681866474448
	140681868259824 [label="layer4.1.conv1.0.1.bias
 (1152)" fillcolor=lightblue]
	140681868259824 -> 140681866476944
	140681866476944 [label=AccumulateGrad]
	140681866468016 -> 140681866474208
	140681868260704 [label="layer4.1.conv1.0.3.weight
 (512, 1152, 3, 1, 1)" fillcolor=lightblue]
	140681868260704 -> 140681866468016
	140681866468016 [label=AccumulateGrad]
	140681866470896 -> 140681866468640
	140681868260944 [label="layer4.1.conv1.1.weight
 (512)" fillcolor=lightblue]
	140681868260944 -> 140681866470896
	140681866470896 [label=AccumulateGrad]
	140681866466720 -> 140681866468640
	140681868260304 [label="layer4.1.conv1.1.bias
 (512)" fillcolor=lightblue]
	140681868260304 -> 140681866466720
	140681866466720 [label=AccumulateGrad]
	140681866473632 -> 140681866475264
	140681868261424 [label="layer4.1.conv2.0.0.weight
 (1152, 512, 1, 3, 3)" fillcolor=lightblue]
	140681868261424 -> 140681866473632
	140681866473632 [label=AccumulateGrad]
	140681866469264 -> 140681866472096
	140681868260144 [label="layer4.1.conv2.0.1.weight
 (1152)" fillcolor=lightblue]
	140681868260144 -> 140681866469264
	140681866469264 [label=AccumulateGrad]
	140681866470800 -> 140681866472096
	140681868261024 [label="layer4.1.conv2.0.1.bias
 (1152)" fillcolor=lightblue]
	140681868261024 -> 140681866470800
	140681866470800 [label=AccumulateGrad]
	140681866470416 -> 140681866471376
	140681868260544 [label="layer4.1.conv2.0.3.weight
 (512, 1152, 3, 1, 1)" fillcolor=lightblue]
	140681868260544 -> 140681866470416
	140681866470416 [label=AccumulateGrad]
	140681866469168 -> 140681866476656
	140681868261744 [label="layer4.1.conv2.1.weight
 (512)" fillcolor=lightblue]
	140681868261744 -> 140681866469168
	140681866469168 [label=AccumulateGrad]
	140681866468736 -> 140681866476656
	140681868262384 [label="layer4.1.conv2.1.bias
 (512)" fillcolor=lightblue]
	140681868262384 -> 140681866468736
	140681866468736 [label=AccumulateGrad]
	140681866469888 -> 140681866471952
	140681866462976 -> 140681923932416
	140681866462976 [label=TBackward0]
	140681866463504 -> 140681866462976
	140681868260624 [label="fc.weight
 (400, 512)" fillcolor=lightblue]
	140681868260624 -> 140681866463504
	140681866463504 [label=AccumulateGrad]
	140681923932416 -> 140681867942928
}
