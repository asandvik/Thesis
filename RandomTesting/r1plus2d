digraph {
	graph [size="105.14999999999999,105.14999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140681869380320 [label="
 (1, 400)" fillcolor=darkolivegreen1]
	140681866469408 [label=AddmmBackward0]
	140689846039072 -> 140681866469408
	140681918808688 [label="fc.bias
 (400)" fillcolor=lightblue]
	140681918808688 -> 140689846039072
	140689846039072 [label=AccumulateGrad]
	140681866469456 -> 140681866469408
	140681866469456 [label=ViewBackward0]
	140681866469504 -> 140681866469456
	140681866469504 [label=MeanBackward1]
	140681866471568 -> 140681866469504
	140681866471568 [label=ReluBackward0]
	140681866471664 -> 140681866471568
	140681866471664 [label=AddBackward0]
	140681866471760 -> 140681866471664
	140681866471760 [label=CudnnBatchNormBackward0]
	140681866472768 -> 140681866471760
	140681866472768 [label=ConvolutionBackward0]
	140681866476896 -> 140681866472768
	140681866476896 [label=ReluBackward0]
	140681866473776 -> 140681866476896
	140681866473776 [label=CudnnBatchNormBackward0]
	140681866473680 -> 140681866473776
	140681866473680 [label=ConvolutionBackward0]
	140681866473584 -> 140681866473680
	140681866473584 [label=ReluBackward0]
	140681866473488 -> 140681866473584
	140681866473488 [label=CudnnBatchNormBackward0]
	140681866473392 -> 140681866473488
	140681866473392 [label=ConvolutionBackward0]
	140681866473296 -> 140681866473392
	140681866473296 [label=ReluBackward0]
	140681866473200 -> 140681866473296
	140681866473200 [label=CudnnBatchNormBackward0]
	140681866473104 -> 140681866473200
	140681866473104 [label=ConvolutionBackward0]
	140681866471712 -> 140681866473104
	140681866471712 [label=ReluBackward0]
	140681866472960 -> 140681866471712
	140681866472960 [label=AddBackward0]
	140681866472864 -> 140681866472960
	140681866472864 [label=CudnnBatchNormBackward0]
	140681866471424 -> 140681866472864
	140681866471424 [label=ConvolutionBackward0]
	140681866471328 -> 140681866471424
	140681866471328 [label=ReluBackward0]
	140681866471232 -> 140681866471328
	140681866471232 [label=CudnnBatchNormBackward0]
	140681866471136 -> 140681866471232
	140681866471136 [label=ConvolutionBackward0]
	140681866471040 -> 140681866471136
	140681866471040 [label=ReluBackward0]
	140681866470944 -> 140681866471040
	140681866470944 [label=CudnnBatchNormBackward0]
	140681866470848 -> 140681866470944
	140681866470848 [label=ConvolutionBackward0]
	140681866470752 -> 140681866470848
	140681866470752 [label=ReluBackward0]
	140681866470656 -> 140681866470752
	140681866470656 [label=CudnnBatchNormBackward0]
	140681866470560 -> 140681866470656
	140681866470560 [label=ConvolutionBackward0]
	140681866470128 -> 140681866470560
	140681866470128 [label=ReluBackward0]
	140681866470032 -> 140681866470128
	140681866470032 [label=AddBackward0]
	140681866469936 -> 140681866470032
	140681866469936 [label=CudnnBatchNormBackward0]
	140681866469792 -> 140681866469936
	140681866469792 [label=ConvolutionBackward0]
	140681866469696 -> 140681866469792
	140681866469696 [label=ReluBackward0]
	140681866469120 -> 140681866469696
	140681866469120 [label=CudnnBatchNormBackward0]
	140681866469024 -> 140681866469120
	140681866469024 [label=ConvolutionBackward0]
	140681866468928 -> 140681866469024
	140681866468928 [label=ReluBackward0]
	140681866468784 -> 140681866468928
	140681866468784 [label=CudnnBatchNormBackward0]
	140681866468688 -> 140681866468784
	140681866468688 [label=ConvolutionBackward0]
	140681866468448 -> 140681866468688
	140681866468448 [label=ReluBackward0]
	140681866468496 -> 140681866468448
	140681866468496 [label=CudnnBatchNormBackward0]
	140681866475600 -> 140681866468496
	140681866475600 [label=ConvolutionBackward0]
	140681866469984 -> 140681866475600
	140681866469984 [label=ReluBackward0]
	140681866472480 -> 140681866469984
	140681866472480 [label=AddBackward0]
	140681866472576 -> 140681866472480
	140681866472576 [label=CudnnBatchNormBackward0]
	140681866472720 -> 140681866472576
	140681866472720 [label=ConvolutionBackward0]
	140681866474400 -> 140681866472720
	140681866474400 [label=ReluBackward0]
	140681866474496 -> 140681866474400
	140681866474496 [label=CudnnBatchNormBackward0]
	140681866475408 -> 140681866474496
	140681866475408 [label=ConvolutionBackward0]
	140681866475456 -> 140681866475408
	140681866475456 [label=ReluBackward0]
	140681866476800 -> 140681866475456
	140681866476800 [label=CudnnBatchNormBackward0]
	140681866476704 -> 140681866476800
	140681866476704 [label=ConvolutionBackward0]
	140681866476608 -> 140681866476704
	140681866476608 [label=ReluBackward0]
	140681866476512 -> 140681866476608
	140681866476512 [label=CudnnBatchNormBackward0]
	140681866476416 -> 140681866476512
	140681866476416 [label=ConvolutionBackward0]
	140681866476320 -> 140681866476416
	140681866476320 [label=ReluBackward0]
	140681866476224 -> 140681866476320
	140681866476224 [label=AddBackward0]
	140681866476128 -> 140681866476224
	140681866476128 [label=CudnnBatchNormBackward0]
	140681866475984 -> 140681866476128
	140681866475984 [label=ConvolutionBackward0]
	140681866475888 -> 140681866475984
	140681866475888 [label=ReluBackward0]
	140681866475792 -> 140681866475888
	140681866475792 [label=CudnnBatchNormBackward0]
	140681866465664 -> 140681866475792
	140681866465664 [label=ConvolutionBackward0]
	140681866465568 -> 140681866465664
	140681866465568 [label=ReluBackward0]
	140681866465472 -> 140681866465568
	140681866465472 [label=CudnnBatchNormBackward0]
	140681866465376 -> 140681866465472
	140681866465376 [label=ConvolutionBackward0]
	140681866465280 -> 140681866465376
	140681866465280 [label=ReluBackward0]
	140681866465184 -> 140681866465280
	140681866465184 [label=CudnnBatchNormBackward0]
	140681866465088 -> 140681866465184
	140681866465088 [label=ConvolutionBackward0]
	140681866476176 -> 140681866465088
	140681866476176 [label=ReluBackward0]
	140681866464944 -> 140681866476176
	140681866464944 [label=AddBackward0]
	140681866464848 -> 140681866464944
	140681866464848 [label=CudnnBatchNormBackward0]
	140681866464704 -> 140681866464848
	140681866464704 [label=ConvolutionBackward0]
	140681866464608 -> 140681866464704
	140681866464608 [label=ReluBackward0]
	140681866464512 -> 140681866464608
	140681866464512 [label=CudnnBatchNormBackward0]
	140681866464416 -> 140681866464512
	140681866464416 [label=ConvolutionBackward0]
	140681866464320 -> 140681866464416
	140681866464320 [label=ReluBackward0]
	140681866464224 -> 140681866464320
	140681866464224 [label=CudnnBatchNormBackward0]
	140681866464128 -> 140681866464224
	140681866464128 [label=ConvolutionBackward0]
	140681866464032 -> 140681866464128
	140681866464032 [label=ReluBackward0]
	140681866463936 -> 140681866464032
	140681866463936 [label=CudnnBatchNormBackward0]
	140681866463840 -> 140681866463936
	140681866463840 [label=ConvolutionBackward0]
	140681866463744 -> 140681866463840
	140681866463744 [label=ReluBackward0]
	140681866463648 -> 140681866463744
	140681866463648 [label=AddBackward0]
	140681866463552 -> 140681866463648
	140681866463552 [label=CudnnBatchNormBackward0]
	140681866463408 -> 140681866463552
	140681866463408 [label=ConvolutionBackward0]
	140681866463312 -> 140681866463408
	140681866463312 [label=ReluBackward0]
	140681866463216 -> 140681866463312
	140681866463216 [label=CudnnBatchNormBackward0]
	140681866463120 -> 140681866463216
	140681866463120 [label=ConvolutionBackward0]
	140681866463024 -> 140681866463120
	140681866463024 [label=ReluBackward0]
	140681866462928 -> 140681866463024
	140681866462928 [label=CudnnBatchNormBackward0]
	140681866462832 -> 140681866462928
	140681866462832 [label=ConvolutionBackward0]
	140681866462736 -> 140681866462832
	140681866462736 [label=ReluBackward0]
	140681866462640 -> 140681866462736
	140681866462640 [label=CudnnBatchNormBackward0]
	140681866462544 -> 140681866462640
	140681866462544 [label=ConvolutionBackward0]
	140681866463600 -> 140681866462544
	140681866463600 [label=ReluBackward0]
	140681866462400 -> 140681866463600
	140681866462400 [label=AddBackward0]
	140681866462304 -> 140681866462400
	140681866462304 [label=CudnnBatchNormBackward0]
	140681866462160 -> 140681866462304
	140681866462160 [label=ConvolutionBackward0]
	140681866462064 -> 140681866462160
	140681866462064 [label=ReluBackward0]
	140681866461968 -> 140681866462064
	140681866461968 [label=CudnnBatchNormBackward0]
	140681866461872 -> 140681866461968
	140681866461872 [label=ConvolutionBackward0]
	140681866461776 -> 140681866461872
	140681866461776 [label=ReluBackward0]
	140681866461680 -> 140681866461776
	140681866461680 [label=CudnnBatchNormBackward0]
	140681866461584 -> 140681866461680
	140681866461584 [label=ConvolutionBackward0]
	140681866461488 -> 140681866461584
	140681866461488 [label=ReluBackward0]
	140681866461392 -> 140681866461488
	140681866461392 [label=CudnnBatchNormBackward0]
	140681866461296 -> 140681866461392
	140681866461296 [label=ConvolutionBackward0]
	140681866462352 -> 140681866461296
	140681866462352 [label=ReluBackward0]
	140681866462256 -> 140681866462352
	140681866462256 [label=CudnnBatchNormBackward0]
	140681866413824 -> 140681866462256
	140681866413824 [label=ConvolutionBackward0]
	140681866413920 -> 140681866413824
	140681866413920 [label=ReluBackward0]
	140681866414016 -> 140681866413920
	140681866414016 [label=CudnnBatchNormBackward0]
	140681866414112 -> 140681866414016
	140681866414112 [label=ConvolutionBackward0]
	140681868295424 -> 140681866414112
	140682308196592 [label="stem.0.weight
 (45, 3, 1, 7, 7)" fillcolor=lightblue]
	140682308196592 -> 140681868295424
	140681868295424 [label=AccumulateGrad]
	140681868295184 -> 140681866414016
	140682308196432 [label="stem.1.weight
 (45)" fillcolor=lightblue]
	140682308196432 -> 140681868295184
	140681868295184 [label=AccumulateGrad]
	140681868295040 -> 140681866414016
	140682308196032 [label="stem.1.bias
 (45)" fillcolor=lightblue]
	140682308196032 -> 140681868295040
	140681868295040 [label=AccumulateGrad]
	140681868294944 -> 140681866413824
	140682308195792 [label="stem.3.weight
 (64, 45, 3, 1, 1)" fillcolor=lightblue]
	140682308195792 -> 140681868294944
	140681868294944 [label=AccumulateGrad]
	140681868294752 -> 140681866462256
	140682308195712 [label="stem.4.weight
 (64)" fillcolor=lightblue]
	140682308195712 -> 140681868294752
	140681868294752 [label=AccumulateGrad]
	140681868294512 -> 140681866462256
	140682308195552 [label="stem.4.bias
 (64)" fillcolor=lightblue]
	140682308195552 -> 140681868294512
	140681868294512 [label=AccumulateGrad]
	140681868294608 -> 140681866461296
	140681965352448 [label="layer1.0.conv1.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140681965352448 -> 140681868294608
	140681868294608 [label=AccumulateGrad]
	140681868294368 -> 140681866461392
	140682308195072 [label="layer1.0.conv1.0.1.weight
 (144)" fillcolor=lightblue]
	140682308195072 -> 140681868294368
	140681868294368 [label=AccumulateGrad]
	140681868294224 -> 140681866461392
	140682308194992 [label="layer1.0.conv1.0.1.bias
 (144)" fillcolor=lightblue]
	140682308194992 -> 140681868294224
	140681868294224 [label=AccumulateGrad]
	140681868294128 -> 140681866461584
	140682308194352 [label="layer1.0.conv1.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140682308194352 -> 140681868294128
	140681868294128 [label=AccumulateGrad]
	140681868293936 -> 140681866461680
	140681926391616 [label="layer1.0.conv1.1.weight
 (64)" fillcolor=lightblue]
	140681926391616 -> 140681868293936
	140681868293936 [label=AccumulateGrad]
	140681868293792 -> 140681866461680
	140682308194512 [label="layer1.0.conv1.1.bias
 (64)" fillcolor=lightblue]
	140682308194512 -> 140681868293792
	140681868293792 [label=AccumulateGrad]
	140681868293696 -> 140681866461872
	140682308193872 [label="layer1.0.conv2.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140682308193872 -> 140681868293696
	140681868293696 [label=AccumulateGrad]
	140681868293504 -> 140681866461968
	140682308193792 [label="layer1.0.conv2.0.1.weight
 (144)" fillcolor=lightblue]
	140682308193792 -> 140681868293504
	140681868293504 [label=AccumulateGrad]
	140681868293360 -> 140681866461968
	140682308193632 [label="layer1.0.conv2.0.1.bias
 (144)" fillcolor=lightblue]
	140682308193632 -> 140681868293360
	140681868293360 [label=AccumulateGrad]
	140681868293264 -> 140681866462160
	140682308193072 [label="layer1.0.conv2.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140682308193072 -> 140681868293264
	140681868293264 [label=AccumulateGrad]
	140681868293072 -> 140681866462304
	140682308202912 [label="layer1.0.conv2.1.weight
 (64)" fillcolor=lightblue]
	140682308202912 -> 140681868293072
	140681868293072 [label=AccumulateGrad]
	140681868293024 -> 140681866462304
	140682308202832 [label="layer1.0.conv2.1.bias
 (64)" fillcolor=lightblue]
	140682308202832 -> 140681868293024
	140681868293024 [label=AccumulateGrad]
	140681866462352 -> 140681866462400
	140681868292784 -> 140681866462544
	140682308202512 [label="layer1.1.conv1.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140682308202512 -> 140681868292784
	140681868292784 [label=AccumulateGrad]
	140681868292544 -> 140681866462640
	140682308202432 [label="layer1.1.conv1.0.1.weight
 (144)" fillcolor=lightblue]
	140682308202432 -> 140681868292544
	140681868292544 [label=AccumulateGrad]
	140681868292400 -> 140681866462640
	140682308202352 [label="layer1.1.conv1.0.1.bias
 (144)" fillcolor=lightblue]
	140682308202352 -> 140681868292400
	140681868292400 [label=AccumulateGrad]
	140681868292304 -> 140681866462832
	140682308191552 [label="layer1.1.conv1.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140682308191552 -> 140681868292304
	140681868292304 [label=AccumulateGrad]
	140681868292112 -> 140681866462928
	140682308193392 [label="layer1.1.conv1.1.weight
 (64)" fillcolor=lightblue]
	140682308193392 -> 140681868292112
	140681868292112 [label=AccumulateGrad]
	140681868291968 -> 140681866462928
	140682308192272 [label="layer1.1.conv1.1.bias
 (64)" fillcolor=lightblue]
	140682308192272 -> 140681868291968
	140681868291968 [label=AccumulateGrad]
	140681868291872 -> 140681866463120
	140682308201072 [label="layer1.1.conv2.0.0.weight
 (144, 64, 1, 3, 3)" fillcolor=lightblue]
	140682308201072 -> 140681868291872
	140681868291872 [label=AccumulateGrad]
	140681868291680 -> 140681866463216
	140682308190832 [label="layer1.1.conv2.0.1.weight
 (144)" fillcolor=lightblue]
	140682308190832 -> 140681868291680
	140681868291680 [label=AccumulateGrad]
	140681868291536 -> 140681866463216
	140682308190992 [label="layer1.1.conv2.0.1.bias
 (144)" fillcolor=lightblue]
	140682308190992 -> 140681868291536
	140681868291536 [label=AccumulateGrad]
	140681868291440 -> 140681866463408
	140682308201872 [label="layer1.1.conv2.0.3.weight
 (64, 144, 3, 1, 1)" fillcolor=lightblue]
	140682308201872 -> 140681868291440
	140681868291440 [label=AccumulateGrad]
	140681868291248 -> 140681866463552
	140682308191952 [label="layer1.1.conv2.1.weight
 (64)" fillcolor=lightblue]
	140682308191952 -> 140681868291248
	140681868291248 [label=AccumulateGrad]
	140681868291200 -> 140681866463552
	140682308192112 [label="layer1.1.conv2.1.bias
 (64)" fillcolor=lightblue]
	140682308192112 -> 140681868291200
	140681868291200 [label=AccumulateGrad]
	140681866463600 -> 140681866463648
	140681868290864 -> 140681866463840
	140682308268688 [label="layer2.0.conv1.0.0.weight
 (230, 64, 1, 3, 3)" fillcolor=lightblue]
	140682308268688 -> 140681868290864
	140681868290864 [label=AccumulateGrad]
	140681868290672 -> 140681866463936
	140682308261728 [label="layer2.0.conv1.0.1.weight
 (230)" fillcolor=lightblue]
	140682308261728 -> 140681868290672
	140681868290672 [label=AccumulateGrad]
	140681868290528 -> 140681866463936
	140682308261568 [label="layer2.0.conv1.0.1.bias
 (230)" fillcolor=lightblue]
	140682308261568 -> 140681868290528
	140681868290528 [label=AccumulateGrad]
	140681868290432 -> 140681866464128
	140682308268448 [label="layer2.0.conv1.0.3.weight
 (128, 230, 3, 1, 1)" fillcolor=lightblue]
	140682308268448 -> 140681868290432
	140681868290432 [label=AccumulateGrad]
	140681868290240 -> 140681866464224
	140682308261408 [label="layer2.0.conv1.1.weight
 (128)" fillcolor=lightblue]
	140682308261408 -> 140681868290240
	140681868290240 [label=AccumulateGrad]
	140681868290096 -> 140681866464224
	140682308268368 [label="layer2.0.conv1.1.bias
 (128)" fillcolor=lightblue]
	140682308268368 -> 140681868290096
	140681868290096 [label=AccumulateGrad]
	140681868290000 -> 140681866464416
	140682308261168 [label="layer2.0.conv2.0.0.weight
 (230, 128, 1, 3, 3)" fillcolor=lightblue]
	140682308261168 -> 140681868290000
	140681868290000 [label=AccumulateGrad]
	140681868289808 -> 140681866464512
	140682308261088 [label="layer2.0.conv2.0.1.weight
 (230)" fillcolor=lightblue]
	140682308261088 -> 140681868289808
	140681868289808 [label=AccumulateGrad]
	140681868289664 -> 140681866464512
	140682308268128 [label="layer2.0.conv2.0.1.bias
 (230)" fillcolor=lightblue]
	140682308268128 -> 140681868289664
	140681868289664 [label=AccumulateGrad]
	140681868289568 -> 140681866464704
	140682308267888 [label="layer2.0.conv2.0.3.weight
 (128, 230, 3, 1, 1)" fillcolor=lightblue]
	140682308267888 -> 140681868289568
	140681868289568 [label=AccumulateGrad]
	140681868289376 -> 140681866464848
	140682308260848 [label="layer2.0.conv2.1.weight
 (128)" fillcolor=lightblue]
	140682308260848 -> 140681868289376
	140681868289376 [label=AccumulateGrad]
	140681868289328 -> 140681866464848
	140682308260768 [label="layer2.0.conv2.1.bias
 (128)" fillcolor=lightblue]
	140682308260768 -> 140681868289328
	140681868289328 [label=AccumulateGrad]
	140681866464896 -> 140681866464944
	140681866464896 [label=CudnnBatchNormBackward0]
	140681866464368 -> 140681866464896
	140681866464368 [label=ConvolutionBackward0]
	140681866463744 -> 140681866464368
	140681868290336 -> 140681866464368
	140682308197712 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1, 1)" fillcolor=lightblue]
	140682308197712 -> 140681868290336
	140681868290336 [label=AccumulateGrad]
	140681868289520 -> 140681866464896
	140682308197872 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140682308197872 -> 140681868289520
	140681868289520 [label=AccumulateGrad]
	140681868289472 -> 140681866464896
	140682308197792 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140682308197792 -> 140681868289472
	140681868289472 [label=AccumulateGrad]
	140681868289088 -> 140681866465088
	140682308267488 [label="layer2.1.conv1.0.0.weight
 (288, 128, 1, 3, 3)" fillcolor=lightblue]
	140682308267488 -> 140681868289088
	140681868289088 [label=AccumulateGrad]
	140681868288848 -> 140681866465184
	140682308267408 [label="layer2.1.conv1.0.1.weight
 (288)" fillcolor=lightblue]
	140682308267408 -> 140681868288848
	140681868288848 [label=AccumulateGrad]
	140681868288704 -> 140681866465184
	140682308260688 [label="layer2.1.conv1.0.1.bias
 (288)" fillcolor=lightblue]
	140682308260688 -> 140681868288704
	140681868288704 [label=AccumulateGrad]
	140681868288608 -> 140681866465376
	140682308260288 [label="layer2.1.conv1.0.3.weight
 (128, 288, 3, 1, 1)" fillcolor=lightblue]
	140682308260288 -> 140681868288608
	140681868288608 [label=AccumulateGrad]
	140681868288416 -> 140681866465472
	140682308260368 [label="layer2.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	140682308260368 -> 140681868288416
	140681868288416 [label=AccumulateGrad]
	140681868288272 -> 140681866465472
	140682308267328 [label="layer2.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	140682308267328 -> 140681868288272
	140681868288272 [label=AccumulateGrad]
	140681868288176 -> 140681866465664
	140682308267168 [label="layer2.1.conv2.0.0.weight
 (288, 128, 1, 3, 3)" fillcolor=lightblue]
	140682308267168 -> 140681868288176
	140681868288176 [label=AccumulateGrad]
	140681868287984 -> 140681866475792
	140682308267088 [label="layer2.1.conv2.0.1.weight
 (288)" fillcolor=lightblue]
	140682308267088 -> 140681868287984
	140681868287984 [label=AccumulateGrad]
	140681868287840 -> 140681866475792
	140682308260048 [label="layer2.1.conv2.0.1.bias
 (288)" fillcolor=lightblue]
	140682308260048 -> 140681868287840
	140681868287840 [label=AccumulateGrad]
	140681868287744 -> 140681866475984
	140682308259808 [label="layer2.1.conv2.0.3.weight
 (128, 288, 3, 1, 1)" fillcolor=lightblue]
	140682308259808 -> 140681868287744
	140681868287744 [label=AccumulateGrad]
	140681868287552 -> 140681866476128
	140682308266848 [label="layer2.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	140682308266848 -> 140681868287552
	140681868287552 [label=AccumulateGrad]
	140681868287504 -> 140681866476128
	140682308266768 [label="layer2.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	140682308266768 -> 140681868287504
	140681868287504 [label=AccumulateGrad]
	140681866476176 -> 140681866476224
	140681868287168 -> 140681866476416
	140682308259408 [label="layer3.0.conv1.0.0.weight
 (460, 128, 1, 3, 3)" fillcolor=lightblue]
	140682308259408 -> 140681868287168
	140681868287168 [label=AccumulateGrad]
	140681868286976 -> 140681866476512
	140682308266368 [label="layer3.0.conv1.0.1.weight
 (460)" fillcolor=lightblue]
	140682308266368 -> 140681868286976
	140681868286976 [label=AccumulateGrad]
	140681868286832 -> 140681866476512
	140682308266288 [label="layer3.0.conv1.0.1.bias
 (460)" fillcolor=lightblue]
	140682308266288 -> 140681868286832
	140681868286832 [label=AccumulateGrad]
	140681868286736 -> 140681866476704
	140682308266128 [label="layer3.0.conv1.0.3.weight
 (256, 460, 3, 1, 1)" fillcolor=lightblue]
	140682308266128 -> 140681868286736
	140681868286736 [label=AccumulateGrad]
	140681868286544 -> 140681866476800
	140682308259088 [label="layer3.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	140682308259088 -> 140681868286544
	140681868286544 [label=AccumulateGrad]
	140681868286400 -> 140681866476800
	140682308259008 [label="layer3.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	140682308259008 -> 140681868286400
	140681868286400 [label=AccumulateGrad]
	140681868286304 -> 140681866475408
	140682308258848 [label="layer3.0.conv2.0.0.weight
 (460, 256, 1, 3, 3)" fillcolor=lightblue]
	140682308258848 -> 140681868286304
	140681868286304 [label=AccumulateGrad]
	140681868286112 -> 140681866474496
	140682308265888 [label="layer3.0.conv2.0.1.weight
 (460)" fillcolor=lightblue]
	140682308265888 -> 140681868286112
	140681868286112 [label=AccumulateGrad]
	140681868285920 -> 140681866474496
	140682308265808 [label="layer3.0.conv2.0.1.bias
 (460)" fillcolor=lightblue]
	140682308265808 -> 140681868285920
	140681868285920 [label=AccumulateGrad]
	140681868286016 -> 140681866472720
	140682308258608 [label="layer3.0.conv2.0.3.weight
 (256, 460, 3, 1, 1)" fillcolor=lightblue]
	140682308258608 -> 140681868286016
	140681868286016 [label=AccumulateGrad]
	140681868285632 -> 140681866472576
	140682308258528 [label="layer3.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	140682308258528 -> 140681868285632
	140681868285632 [label=AccumulateGrad]
	140681868285824 -> 140681866472576
	140682308265568 [label="layer3.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	140682308265568 -> 140681868285824
	140681868285824 [label=AccumulateGrad]
	140681866472528 -> 140681866472480
	140681866472528 [label=CudnnBatchNormBackward0]
	140681866475552 -> 140681866472528
	140681866475552 [label=ConvolutionBackward0]
	140681866476320 -> 140681866475552
	140681868286640 -> 140681866475552
	140682308266608 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1, 1)" fillcolor=lightblue]
	140682308266608 -> 140681868286640
	140681868286640 [label=AccumulateGrad]
	140681868285680 -> 140681866472528
	140682308259568 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140682308259568 -> 140681868285680
	140681868285680 [label=AccumulateGrad]
	140681868285776 -> 140681866472528
	140682308259488 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140682308259488 -> 140681868285776
	140681868285776 [label=AccumulateGrad]
	140681868285584 -> 140681866475600
	140682308265168 [label="layer3.1.conv1.0.0.weight
 (576, 256, 1, 3, 3)" fillcolor=lightblue]
	140682308265168 -> 140681868285584
	140681868285584 [label=AccumulateGrad]
	140681868285488 -> 140681866468496
	140682308258448 [label="layer3.1.conv1.0.1.weight
 (576)" fillcolor=lightblue]
	140682308258448 -> 140681868285488
	140681868285488 [label=AccumulateGrad]
	140681868284960 -> 140681866468496
	140682308258368 [label="layer3.1.conv1.0.1.bias
 (576)" fillcolor=lightblue]
	140682308258368 -> 140681868284960
	140681868284960 [label=AccumulateGrad]
	140681868284720 -> 140681866468688
	140682308265088 [label="layer3.1.conv1.0.3.weight
 (256, 576, 3, 1, 1)" fillcolor=lightblue]
	140682308265088 -> 140681868284720
	140681868284720 [label=AccumulateGrad]
	140681868285008 -> 140681866468784
	140682308258048 [label="layer3.1.conv1.1.weight
 (256)" fillcolor=lightblue]
	140682308258048 -> 140681868285008
	140681868285008 [label=AccumulateGrad]
	140681868284864 -> 140681866468784
	140682308265008 [label="layer3.1.conv1.1.bias
 (256)" fillcolor=lightblue]
	140682308265008 -> 140681868284864
	140681868284864 [label=AccumulateGrad]
	140681868284576 -> 140681866469024
	140682308257808 [label="layer3.1.conv2.0.0.weight
 (576, 256, 1, 3, 3)" fillcolor=lightblue]
	140682308257808 -> 140681868284576
	140681868284576 [label=AccumulateGrad]
	140681868284624 -> 140681866469120
	140682308264768 [label="layer3.1.conv2.0.1.weight
 (576)" fillcolor=lightblue]
	140682308264768 -> 140681868284624
	140681868284624 [label=AccumulateGrad]
	140681868283712 -> 140681866469120
	140682308264688 [label="layer3.1.conv2.0.1.bias
 (576)" fillcolor=lightblue]
	140682308264688 -> 140681868283712
	140681868283712 [label=AccumulateGrad]
	140681868283952 -> 140681866469792
	140682308257408 [label="layer3.1.conv2.0.3.weight
 (256, 576, 3, 1, 1)" fillcolor=lightblue]
	140682308257408 -> 140681868283952
	140681868283952 [label=AccumulateGrad]
	140681868283760 -> 140681866469936
	140682308257328 [label="layer3.1.conv2.1.weight
 (256)" fillcolor=lightblue]
	140682308257328 -> 140681868283760
	140681868283760 [label=AccumulateGrad]
	140681868284144 -> 140681866469936
	140682308264288 [label="layer3.1.conv2.1.bias
 (256)" fillcolor=lightblue]
	140682308264288 -> 140681868284144
	140681868284144 [label=AccumulateGrad]
	140681866469984 -> 140681866470032
	140681868283328 -> 140681866470560
	140682308263808 [label="layer4.0.conv1.0.0.weight
 (921, 256, 1, 3, 3)" fillcolor=lightblue]
	140682308263808 -> 140681868283328
	140681868283328 [label=AccumulateGrad]
	140681868283232 -> 140681866470656
	140682308263728 [label="layer4.0.conv1.0.1.weight
 (921)" fillcolor=lightblue]
	140682308263728 -> 140681868283232
	140681868283232 [label=AccumulateGrad]
	140681868282944 -> 140681866470656
	140682308256768 [label="layer4.0.conv1.0.1.bias
 (921)" fillcolor=lightblue]
	140682308256768 -> 140681868282944
	140681868282944 [label=AccumulateGrad]
	140681868282848 -> 140681866470848
	140682308256528 [label="layer4.0.conv1.0.3.weight
 (512, 921, 3, 1, 1)" fillcolor=lightblue]
	140682308256528 -> 140681868282848
	140681868282848 [label=AccumulateGrad]
	140681868280976 -> 140681866470944
	140682308263488 [label="layer4.0.conv1.1.weight
 (512)" fillcolor=lightblue]
	140682308263488 -> 140681868280976
	140681868280976 [label=AccumulateGrad]
	140681868282224 -> 140681866470944
	140682308263408 [label="layer4.0.conv1.1.bias
 (512)" fillcolor=lightblue]
	140682308263408 -> 140681868282224
	140681868282224 [label=AccumulateGrad]
	140681868280592 -> 140681866471136
	140682308261968 [label="layer4.0.conv2.0.0.weight
 (921, 512, 1, 3, 3)" fillcolor=lightblue]
	140682308261968 -> 140681868280592
	140681868280592 [label=AccumulateGrad]
	140681868282320 -> 140681866471232
	140682308261888 [label="layer4.0.conv2.0.1.weight
 (921)" fillcolor=lightblue]
	140682308261888 -> 140681868282320
	140681868282320 [label=AccumulateGrad]
	140681868282992 -> 140681866471232
	140682308268928 [label="layer4.0.conv2.0.1.bias
 (921)" fillcolor=lightblue]
	140682308268928 -> 140681868282992
	140681868282992 [label=AccumulateGrad]
	140681868281744 -> 140681866471424
	140681918707680 [label="layer4.0.conv2.0.3.weight
 (512, 921, 3, 1, 1)" fillcolor=lightblue]
	140681918707680 -> 140681868281744
	140681868281744 [label=AccumulateGrad]
	140681868280544 -> 140681866472864
	140681918707760 [label="layer4.0.conv2.1.weight
 (512)" fillcolor=lightblue]
	140681918707760 -> 140681868280544
	140681868280544 [label=AccumulateGrad]
	140681868281168 -> 140681866472864
	140681918707840 [label="layer4.0.conv2.1.bias
 (512)" fillcolor=lightblue]
	140681918707840 -> 140681868281168
	140681868281168 [label=AccumulateGrad]
	140681866472912 -> 140681866472960
	140681866472912 [label=CudnnBatchNormBackward0]
	140681866471088 -> 140681866472912
	140681866471088 [label=ConvolutionBackward0]
	140681866470128 -> 140681866471088
	140681868282512 -> 140681866471088
	140682308264048 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1, 1)" fillcolor=lightblue]
	140682308264048 -> 140681868282512
	140681868282512 [label=AccumulateGrad]
	140681868282416 -> 140681866472912
	140682308264128 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140682308264128 -> 140681868282416
	140681868282416 [label=AccumulateGrad]
	140681868281840 -> 140681866472912
	140682308257008 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140682308257008 -> 140681868281840
	140681868281840 [label=AccumulateGrad]
	140681868282560 -> 140681866473104
	140681918708240 [label="layer4.1.conv1.0.0.weight
 (1152, 512, 1, 3, 3)" fillcolor=lightblue]
	140681918708240 -> 140681868282560
	140681868282560 [label=AccumulateGrad]
	140681868280160 -> 140681866473200
	140681918708320 [label="layer4.1.conv1.0.1.weight
 (1152)" fillcolor=lightblue]
	140681918708320 -> 140681868280160
	140681868280160 [label=AccumulateGrad]
	140681868281648 -> 140681866473200
	140681918708400 [label="layer4.1.conv1.0.1.bias
 (1152)" fillcolor=lightblue]
	140681918708400 -> 140681868281648
	140681868281648 [label=AccumulateGrad]
	140681868279968 -> 140681866473392
	140681918708800 [label="layer4.1.conv1.0.3.weight
 (512, 1152, 3, 1, 1)" fillcolor=lightblue]
	140681918708800 -> 140681868279968
	140681868279968 [label=AccumulateGrad]
	140681868281504 -> 140681866473488
	140681918708880 [label="layer4.1.conv1.1.weight
 (512)" fillcolor=lightblue]
	140681918708880 -> 140681868281504
	140681868281504 [label=AccumulateGrad]
	140681868281984 -> 140681866473488
	140681918708960 [label="layer4.1.conv1.1.bias
 (512)" fillcolor=lightblue]
	140681918708960 -> 140681868281984
	140681868281984 [label=AccumulateGrad]
	140681868281888 -> 140681866473680
	140681918709360 [label="layer4.1.conv2.0.0.weight
 (1152, 512, 1, 3, 3)" fillcolor=lightblue]
	140681918709360 -> 140681868281888
	140681868281888 [label=AccumulateGrad]
	140681868279920 -> 140681866473776
	140681918709440 [label="layer4.1.conv2.0.1.weight
 (1152)" fillcolor=lightblue]
	140681918709440 -> 140681868279920
	140681868279920 [label=AccumulateGrad]
	140681868280400 -> 140681866473776
	140681918709520 [label="layer4.1.conv2.0.1.bias
 (1152)" fillcolor=lightblue]
	140681918709520 -> 140681868280400
	140681868280400 [label=AccumulateGrad]
	140681921225888 -> 140681866472768
	140681918808288 [label="layer4.1.conv2.0.3.weight
 (512, 1152, 3, 1, 1)" fillcolor=lightblue]
	140681918808288 -> 140681921225888
	140681921225888 [label=AccumulateGrad]
	140681921223392 -> 140681866471760
	140681918808368 [label="layer4.1.conv2.1.weight
 (512)" fillcolor=lightblue]
	140681918808368 -> 140681921223392
	140681921223392 [label=AccumulateGrad]
	140681921229584 -> 140681866471760
	140681918808448 [label="layer4.1.conv2.1.bias
 (512)" fillcolor=lightblue]
	140681918808448 -> 140681921229584
	140681921229584 [label=AccumulateGrad]
	140681866471712 -> 140681866471664
	140681866466432 -> 140681866469408
	140681866466432 [label=TBackward0]
	140682213368080 -> 140681866466432
	140681918808768 [label="fc.weight
 (400, 512)" fillcolor=lightblue]
	140681918808768 -> 140682213368080
	140682213368080 [label=AccumulateGrad]
	140681866469408 -> 140681869380320
}
